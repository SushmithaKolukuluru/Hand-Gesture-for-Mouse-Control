 Hand gesture recognition is an emerging domain in artificial intelligence,
 which makes novel user-to-device interaction possible. In this regard, this
 project describes a hand gesture-controlled virtual mouse that utilizes
 algorithms based on AI to interpret hand gestures and translate them into
 corresponding movements by the mice, then revert those movements as the
 system's output. This system is further intended to provide an alternative
 interface for people who do not use the traditional mouse very easily.
 The solution it proposes is using a camera that takes pictures of the user's hand
 and decoding them by an AI algorithm to determine which gestures have been
 performed. The algorithm has been trained on a large number of hand gesture
 datasets to identify all these movements. Once deciphered, a gesture translates
 to some particular action with the mouse that is performed on a virtual screen.
 The system design is flexible and expandable, so one may easily use any
 environment along with any device. The user will be able to use both dynamic
 or static hand gestures to control all input operations. The feature also includes
 voice commands. In this technique, machine learning and computer vision
 techniques will be used to interpret the given inputs, hand gestures, as well as
 voice without having added hardware.
 The deployment used a CNN with the MediaPipe framework. Such technology
 has far-reaching applications, including conducting potentially hazardous
 work with several devices without using one's hands or serving as an auxiliary
 interface for handicapped users. In general, the hand gesture virtual mouse
 system represents a step forward in human-computer interface development
 and further improves human-to-computer interaction and user access.
 Keywords: Virtual Mouse, Human-Computer Interaction, Gesture
 Recognition, Computer Vision, MediaPipe, OpenCV, Machine Learning.
